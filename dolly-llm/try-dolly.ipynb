{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36886266-f777-4fec-bf41-8b501c9b5ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch==1.13.1 in /opt/app-root/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from torch==1.13.1) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/app-root/lib/python3.9/site-packages (from torch==1.13.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch==1.13.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/app-root/lib/python3.9/site-packages (from torch==1.13.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch==1.13.1) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.6.0)\n",
      "Requirement already satisfied: wheel in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.38.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /opt/app-root/lib/python3.9/site-packages (4.29.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: fsspec in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/app-root/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/bin/sh: line 1: 5: No such file or directory\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: accelerate in /opt/app-root/lib/python3.9/site-packages (0.19.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib/python3.9/site-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/app-root/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: wheel in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (67.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pprint (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pprint\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.13.1\n",
    "!pip install transformers\n",
    "!pip install transformers[torch]>=4.28.1,<5\n",
    "!pip install accelerate\n",
    "!pip install pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9cbfe0-14ea-405a-8979-615d00c24b7c",
   "metadata": {},
   "source": [
    "suggested_models = [\n",
    "    \"databricks/dolly-v1-6b\",\n",
    "    \"databricks/dolly-v2-3b\",\n",
    "    \"databricks/dolly-v2-7b\",\n",
    "    \"databricks/dolly-v2-12b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a324cfb1-6a9c-44f5-9f85-5c127781ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045febb8-e8d0-4f2a-b89b-64098267383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.cuda.get_device_name(0)\n",
    "  print(f'GPU device name: {device}')\n",
    "else:\n",
    "  print('No GPU available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976bee22-a15d-4732-b388-b8d2fc24c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_pipeline = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4539efec-fc2c-4409-8108-fd84535d6a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Sylfestheid vom Talbrunn am 13. April 1774\\n\\nMein liebes Freund,\\nSehr geehrte Patient,\\nNun bin ich nachdem Ihrer letzten Behandlung in Hamburg hier eingezogen, um sie wieder beizufangen. Die Dauer dieser Behandlung war mir wohl unerwartet schwer, weil ich mich auf dem Kampf gegen eine saftige Kehlenwunde befinde. Meine Versicherung, die meiner Angebot nach einwandfrei sein soll, daß ich in Dänemark ausmach, wahrhaftig ist. Wird diese Woche also in Talbrunn erlebt, so daß ich die letzte Handlung durchführen kann?\\n\\nAuf Wiedersehen\\nWolfgang Goethe'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_pipeline(\"Write a letter from Wolfgang Goethe to Friedrich Schiller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fa90d1a-0697-49ea-84df-89d3f083c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 0 ns, total: 12.5 s\n",
      "Wall time: 12.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'There once was a young girl from the streets of King’s Landing, who swore she’d one day prove her father was the true rightful king.\\n\\nShe sat upon a stone upon the steps of the Great Pyramid of Giza, staring up at the majesty of the great stone monument in the darkness. The stars twinkled in the sky above and the slaves had lit great bonfires on the sand below to keep themselves warm against the bitter night.\\n\\nThe girl looked down at her hands upon the stone, no longer fit to bear the weight of the crown her father wore upon his brow.\\n\\n“The blood of the iron Throne flows in your veins, father,” she whispered. “There is no room for misgivings in your legacy. Take up your rightful place upon the world as the rightful king of the Seven Kingdoms of my domain!”\\n\\nSlowly, the girl’s tears began to fall down upon the stone beneath her. She began to cry for all she lacked, for all she would never be.\\n\\n“No… no, I want to be a queen, but I don’t want to leave you behind… I want to be with you… I love you…'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "instruct_pipeline(\"Write a very long dialogue between Game of Thrones characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397a76d-6359-4371-8f27-6a343777b3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d49daaae-52ce-45fb-9719-d779e6c35267",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_pipeline_local = pipeline(model=\"./dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f8a0ae2-f96b-4efa-8d9b-b1008812640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction=\"Write a movie script for Game of Thrones season 8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e234aa42-56ae-45c3-b955-20cf48e6c7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Write a movie script for Game of Thrones season 8\n",
      "\n",
      "[{'generated_text': \"* Topic: Daenerys Targaryen comes to Westeros\\n* Description: Daenerys Targaryen, the Khaleesi of the Sons of the Dragon, has returned to the Iron Throne of Westeros. After seizing her father's Kingship and igniting a war against the throne, she will stop at nothing to defend her people. If she becomes a threat to herself, she will destroy everything and everyone in her path.\\n* Action: Daenerys prepares for war. She gathers an army to take back the Iron Throne. She spends years planning and preparing, from acquiring a fleet of ships to building catapults and trebuchets.\"}]\n",
      "\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = instruct_pipeline_local(instruction)\n",
    "if response:\n",
    "    print(f\"Instruction: {instruction}\\n\\n{response}\\n\\n-----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "825e2146-b4ff-43c9-9d99-cd253c5f90a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/pipelines/base.py:1080: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 0 ns, total: 12.6 s\n",
      "Wall time: 12.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, how may I help you?\\n\\nGrey Worm: I am looking to acquire the services of a brave stonemason. Perhaps you may have heard of my sister, Hela, who specializes in such work. \\n\\nAerys Targaryen: Perhaps, but I am afraid I am currently in the market for a more advanced mason. Would you be willing to undertake a preliminary survey?\\n\\nGW: Certainly. I have to say, I am slightly unnerved by the prospect of undertaking something as ostensibly unspectacular as bricklaying, however I have undertaken a great many unspectacular things in my time, including but not limited to:\\n1. building a career as a kitchen worker\\n2. rebuilding an A Song of Ice and Fire set after Season 6 into a slightly less out of date presentation\\n3. attempting to replace a busted elevator cable with wire wool and electrical tape\\n4. swallowing irritating amounts of medicine to make sure I never again need to undergo bladder surgery\\n\\nGW: Indeed, none of those things were particularly taxing or complex. I imagine that your instructions will be quite straightforward. Assuming that I do as requested, when shall I expect payment?\\n\\nAT:'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "instruct_pipeline(\"Write a very long dialogue between Game of Thrones characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50795778-d95c-4378-a72c-4a7230130718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Nuclear fission, in which two or more atoms split in two, releases a lot of energy. For example, the U.S. hydrogen bomb that was exploded over Hiroshima in August 1945, used 90 kilos of TNT worth of fission.\\n\\nNuclear fusion, in which two or more atoms join together to form one or more atoms, releases even more energy than fission. Nuclear fusion, also known as hydrogen fusion, uses hydrogen to create energy. Nuclear fusion releases more energy than any other method of energy creation. For example, the power contained in the nucleus of a hydrogen atom is equivalent to that released in 300,000 to 400,000 tons of TNT. Nuclear fusion can create many forms of energy, from simple lighting to more complex forms such as fusion reactors used to create electricity.'}]\n",
      "[{'generated_text': \"Daenerys: Are you still here?\\nViserys: I could say the same of you.\\nDaenerys: This is your fault. You gave me thatbox of precious stones. I was going to use them to make a throne for myself.\\nViserys: My what?\\nDaenerys: A throne of gold and precious stones like yourbox.\\nViserys: I don't understand.\\nDaenerys: The further I go in my quest to become the greatest queen who ever lived, the more I realise I don't need more stuff.\\nViserys: You mean more people?\\nDaenerys: Even more. I mean people who can carry the stuff I will need. In my travelling pyramid.\\nViserys: Oh.\\nDaenerys: Do you have any ideas?\\nViserys: \\nDaenerys: You're so precious. I'd love to have a few.\\nViserys: It will be our secret.\\nDaenerys: I already know your secret.\\nViserys: You do? What is it?\\nDaenerys: I have a new favorite song.\\nViserys: Oh, it's a song?\"}]\n"
     ]
    }
   ],
   "source": [
    "instructions = [\n",
    "    \"Explain to me the difference between nuclear fission and fusion.\",\n",
    "    \"Write a long dialogue between Game of Thrones characters\",\n",
    "]\n",
    "response = [{'generated_text': 'Nuclear fission occurs when a subatomic particle called an atom splits in two, releasing energy. The two protons and electron from a typical uranium atom for example, retain most of the kinetic energy they had at creation, and if they hit an object with sufficient energy can cause a big enough explosion. Nuclear fusion on the other hand, is the process by which atomic nuclei merge, most often to produce even more atomic nuclei. Fusion gives off energy as heat, whereas fission releases energy as electricity.'}]\n",
    "\n",
    "%%time\n",
    "# Use the model to generate responses for each of the instructions above.\n",
    "for instruction in instructions:\n",
    "    response = instruct_pipeline(instruction)\n",
    "    if response:\n",
    "        generated_text = response[0]['generated_text']\n",
    "        ## print(f\"Instruction: {instruction}\\n\\n\")\n",
    "        pprint.pprint(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b2874-c416-4a29-a97f-a4c092c118b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
